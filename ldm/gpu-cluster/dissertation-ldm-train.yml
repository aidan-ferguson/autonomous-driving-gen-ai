# Job definition for the SoCS OpenCluster GPU cluster
apiVersion: batch/v1
kind: Job
metadata:
  name: dissertation-ldm-train
  namespace: l423aidanproject
# this is where you define the content of the Job
spec:
  # this controls how many times the pod created to run the container defined
  # below will be restarted if an error occurs. By default the container will
  # be restarted up to 6 times which probably isn't what you want!
  backoffLimit: 0
  template:        
    metadata:
      name: dissertation-ldm-train
    spec:
      # in the "containers:" section you define the container(s) that
      # will run inside the Pod itself. Usually you'll just need one. 
      containers:
        # set a name for the container, which will be visible in the
        # CLI/web interface
      - name: dissertation-ldm-train-container
        # This container is based off of the PyTorch version we are using
        # additionally, it has the requirements.txt dependencies pre-installed
        # to save time on the node
        image: aidanferguson1/dissertation:latest
        # Run the training script
        command: 
          - "python3"
          - "/nfs/dissertation/ldm/ldm/Trainer.py"
          - "/nfs/parsed_fsoco"
        resources:
          # Hardware requirement requests
          requests:
            # How many milli-cores we request
            cpu: "2000m" 
            memory: "2Gi"
            # How many GPUs we request
            nvidia.com/gpu: 1
          # Limits, which if exceeded may lead to the container being killed
          limits:
            cpu: "4000m" 
            memory: "8Gi"
            nvidia.com/gpu: 1
        # Where to mount the nfs folder
        volumeMounts:
        - mountPath: /nfs
          name: nfs-access
        # example of defining an environment variable and its value, so that they
        # will be visible inside this container
        env:
        - name: HF_HOME
          value: "/nfs/.cache"
        - name: GPU_CLUSTER
          value: "true"
      # this defines a volume called nfs-access which corresponds to your cluster
      # filespace. 
      volumes:
      - name: nfs-access
        persistentVolumeClaim: 
          # Name of the volume claim to mount
          claimName: l423aidanvol1claim
      # in some cases you will want to run your job on a node with a specific type of
      # GPU. the nodeSelector section allows you to do this. The compute nodes each
      # have an annotation indicating the type of GPU they contain. The 2 lines below
      # tell the Kubernetes scheduler that this job must be scheduled on a node
      # where the value of the "node-role.ida/gpu2080ti" annotation is true, i.e. on
      # a node with RTX 2080 Ti GPUs. Alternative values for this are:
      #  "node-role.ida/gputitan" (Titan RTX)
      #  "node-role.ida/gpu3090" (RTX 3090)
      #  "node-role.ida/gpua6000" (RTX A6000)
      nodeSelector:
        node-role.ida/gputitan: "true"
        # node-role.ida/gpu2080ti: "true"
        # node-role.ida/gpu3090: "true"
        # node-role.ida/gpua6000: "true"
      # On job failure, don't restart the job
      restartPolicy: Never
